{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1193a942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval-Augmented Generation (RAG) System \n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFLoader \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains.conversation.memory import ConversationBufferWindowMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "545dec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants\n",
    "# OpenAI API key\n",
    "OPENAI_API_KEY = \"sk..\"  \n",
    "# provide pdf file path\n",
    "PDF_PATH = \"/Users/ANDHAWAN/Downloads/pyProjects/LLM QA/Benefits2024.pdf\" \n",
    "#local directory path for vector db to store embeddings\n",
    "VECTOR_DB_DIRECTORY = \"/Users/ANDHAWAN/Downloads/pyProjects/LLM QA\"\n",
    "#gpt model you want to use\n",
    "GPT_MODEL_NAME = 'gpt-4'\n",
    "\n",
    "#performance parameters\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "03678ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_split_document(pdf_path):\n",
    "    #loads and splits the document into pages.\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    return loader.load_and_split()\n",
    "\n",
    "def split_text_into_chunks(pages, chunk_size, chunk_overlap):\n",
    "    #splits text into chunks\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(pages)\n",
    "\n",
    "def create_embeddings(api_key):\n",
    "    #initializes embeddings\n",
    "    return OpenAIEmbeddings(openai_api_key=api_key)\n",
    "\n",
    "def setup_vector_database(documents, embeddings, directory):\n",
    "    #sets up a vector database to store embeddings\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    return Chroma.from_documents(documents=documents, embedding=embeddings, persist_directory=directory)\n",
    "\n",
    "def initialize_chat_model(api_key, model_name):\n",
    "    #initializes openaichat model\n",
    "    return ChatOpenAI(openai_api_key=api_key, model_name=model_name, temperature=0.0)\n",
    "\n",
    "def create_retrieval_qa_chain(chat_model, vector_database):\n",
    "    #creates a retrieval QA chain combining model and database.\"\"\"\n",
    "    memory = ConversationBufferWindowMemory(memory_key='chat_history', k=5, return_messages=True)\n",
    "    return ConversationalRetrievalChain.from_llm(chat_model, retriever=vector_database.as_retriever(), memory=memory)\n",
    "\n",
    "def ask_question_and_get_answer(qa_chain, question):\n",
    "    #asks questions\n",
    "    return qa_chain({\"question\": question})['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6efe3d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "pages = load_and_split_document(PDF_PATH)\n",
    "documents = split_text_into_chunks(pages, CHUNK_SIZE, CHUNK_OVERLAP)\n",
    "embeddings = create_embeddings(OPENAI_API_KEY)\n",
    "vector_database = setup_vector_database(documents, embeddings, VECTOR_DB_DIRECTORY)\n",
    "chat_model = initialize_chat_model(OPENAI_API_KEY, GPT_MODEL_NAME)\n",
    "qa_chain = create_retrieval_qa_chain(chat_model, vector_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c78af075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: whed will Gypass be available\n",
      "Answer: Gympass will be available beginning on January 1, 2024.\n",
      "\n",
      "Question: How much is frame allowence\n",
      "Answer: The frame allowance for Vision Plan I will increase from $150 to $200 and for Vision Plan II from $200 to $300.\n",
      "\n",
      "Question: How many vision plans are available\n",
      "Answer: There are two available vision plans.\n",
      "\n",
      "Question: do we have leagl insurance\n",
      "Answer: Yes, ARAG is providing legal insurance services. They are expanding their services in 2024 to include more support for elder law issues such as Medicare eligibility or Social Security, and preparation of legal documents like deeds, mortgages, affidavits, wills, and power of attorney for your aging parents and grandparents. They also offer help in assessing eldercare needs and developing care plans.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "questions = [\n",
    "    \"when will Gypass be available\",\n",
    "    \"How much is frame allowence\",\n",
    "    \"How many vision plans are available\",\n",
    "    \"do we have leagl insurance\"\n",
    "]\n",
    "\n",
    "# Process Questions and Answers\n",
    "for question in questions:\n",
    "    answer = ask_question_and_get_answer(qa_chain, question)\n",
    "    print(f\"Question: {question}\\nAnswer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c187d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aa0b1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
